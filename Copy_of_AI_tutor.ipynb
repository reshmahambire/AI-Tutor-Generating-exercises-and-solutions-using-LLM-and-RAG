{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reshmahambire/AI-Tutor-Generating-exercises-and-solutions-using-LLM-and-RAG/blob/main/Copy_of_AI_tutor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HRSqkIHJ3zsv",
        "outputId": "f668adc0-58da-481c-e437-fdbeb4c5c1c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.59.9)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.32 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.32)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.32->langchain) (24.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.32->langchain) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain openai\n",
        "!pip install langchain-community\n",
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Add a PDF Document Retrieval System"
      ],
      "metadata": {
        "id": "LBcVwrkardcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz #PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "#Upload a PDF\n",
        "pdf_text = extract_text_from_pdf(\"/content/NIPS-2017-attention-is-all-you-need-Paper.pdf\")\n",
        "print(pdf_text[:1000])  # Print the first 1000 characters of extracted text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmPCH__Drb-m",
        "outputId": "6a01c1aa-77ae-45b2-cf87-8c161be0d2fd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Is All You Need\n",
            "Ashish Vaswani∗\n",
            "Google Brain\n",
            "avaswani@google.com\n",
            "Noam Shazeer∗\n",
            "Google Brain\n",
            "noam@google.com\n",
            "Niki Parmar∗\n",
            "Google Research\n",
            "nikip@google.com\n",
            "Jakob Uszkoreit∗\n",
            "Google Research\n",
            "usz@google.com\n",
            "Llion Jones∗\n",
            "Google Research\n",
            "llion@google.com\n",
            "Aidan N. Gomez∗†\n",
            "University of Toronto\n",
            "aidan@cs.toronto.edu\n",
            "Łukasz Kaiser∗\n",
            "Google Brain\n",
            "lukaszkaiser@google.com\n",
            "Illia Polosukhin∗‡\n",
            "illia.polosukhin@gmail.com\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or\n",
            "convolutional neural networks that include an encoder and a decoder. The best\n",
            "performing models also connect the encoder and decoder through an attention\n",
            "mechanism. We propose a new simple network architecture, the Transformer,\n",
            "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
            "entirely. Experiments on two machine translation tasks show these models to\n",
            "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
            "less time to train. Our model a\n",
            "662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "y3ZmR4pj0GRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = pdf_text.replace(\"\\n\",\" \")"
      ],
      "metadata": {
        "id": "BnctH7FrRTiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC6yNaxTogAS",
        "outputId": "e941ff92-b4ea-4c85-baa9-ab20cb3cc952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Is All You Need Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com Jakob Uszkoreit∗ Google Research usz@google.com Llion Jones∗ Google Research llion@google.com Aidan N. Gomez∗† University of Toronto aidan@cs.toronto.edu Łukasz Kaiser∗ Google Brain lukaszkaiser@google.com Illia Polosukhin∗‡ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "V4r1vfmj0I6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "_LmtOMJ0ytGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Embedding"
      ],
      "metadata": {
        "id": "7l3ttcUm0V_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load embedding model\n",
        "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "\n",
        "# Extracted PDF text\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_text(text)\n",
        "\n",
        "# Create chunk embeddings once\n",
        "chunk_embeddings = np.array(embeddings.embed_documents(chunks))"
      ],
      "metadata": {
        "id": "B6h8BqdUxo8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cosine Similarity"
      ],
      "metadata": {
        "id": "BqKiYDlJ0if8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform cosine similarity search\n",
        "def search_document(query, top_k=1, threshold=0.5):\n",
        "    # Embed the query\n",
        "    query_embedding = np.array(embeddings.embed_query(query)).reshape(1, -1)\n",
        "\n",
        "    # Compute cosine similarity with all chunks\n",
        "    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]\n",
        "\n",
        "    # Get top-k results above the threshold\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    results = [chunks[i] for i in top_indices if similarities[i] > threshold]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "tJvwADa_yMPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summarizer Model"
      ],
      "metadata": {
        "id": "06T5WJjN0pdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize summarizer here\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnmFTXJQyQaN",
        "outputId": "1c3fa1d2-3067-4c8b-ebe8-f0e8ff816dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Query"
      ],
      "metadata": {
        "id": "Vlj5AcRn0t-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the search\n",
        "query = \"What is multi-head attention?\"\n",
        "results = search_document(query)"
      ],
      "metadata": {
        "id": "xWzT4r3LyXk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating Output"
      ],
      "metadata": {
        "id": "90iEPj470xId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize all results\n",
        "for text in results:\n",
        "    summary = summarizer(text, do_sample=False)\n",
        "\n",
        "    print(f\"\\nSummary:\\n{summary[0]['summary_text']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98bfNtvNydwe",
        "outputId": "540e7a45-b4ed-40a0-b679-a55f227c6fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 142, but your input_length is only 107. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=53)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary:\n",
            "In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions. This makes it more difﬁcult to learn dependencies between distant positions. We counteract this effect with Multi-Head Attention as described in section 3.2.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}